---
layout: default
title: "远离纯粹的Vibe coding: AI作为副驾驶"
nav_order: 4
---
假设现在你已经使用AI成功构建好了你软件项目的第一个版本，你使用PRD、子任务文档和预设的规则幸运地在几次或一次交互中生成代码、解决bug/issue、启动服务。

但是不幸的是，至此，您纯粹的 Vibe coding 之旅就要结束了。之后，*你必须逐渐花费更多的脑力从AI手里夺过方向盘，坐到主驾驶的位置。*

我们希望通过分享下面的几点或深层或显而易见的思考，来说明你必须让 AI 回到副驾驶位置的原因：

> 🌑 思考-1
>1. **上下文窗口永远显得不足**   
>    无论是 32k、64k、256k、100 万 tokens，还是更大，提示工程师们总会希望塞入更多代码与上下文。过去我们只能放两三个文件，将来可能会希望一次性放入几十甚至上百个文件。
>    你不可能把整个代码库的代码全部给LLM，而LLM也很难准确地检索到你想要修改或者审查的代码位置。   
>2. **LLM在软件设计上远达不到人类专家水平**
>    这一方面来自于LLM上下文窗口不足（无法看到并分析代码库的全貌），另一方面源于其缺乏真正的架构性思维与抽象能力。   
>    人类专家在软件设计时会考虑长期演进、团队协作、可维护性、性能权衡、安全性与合规性等多重因素，而 LLM 更像是基于已有数据和模式的“局部补全器”。这意味着 LLM 擅长的是战术层面的代码生成与修改，而在战略层面的架构设计、模式选型、需求折中、权衡取舍方面，它仍然无法达到人类的深度与远见，结果是：LLM 生成的代码往往可用，但可能导致架构上的碎片化、重复造轮子、缺乏一致性和统一标准。
>3. **维持项目稳定并持续测试和演进需要人来主导**
>    一方面，AI 的幻觉和输出的不确定性，将轻易地破坏项目的稳定性，例如已经实现的功能。尤其是当我们把修改、测试、执行的权限AI全部交给AI时，这种担忧更为突出。（例如，为了测试通过，（绝大部分）LLM都倾向于给很简单的测试用例或者直接修改被测试代码来适配测试用例 🫠）   
>    另一方面，项目后期面临的不再是“能不能跑起来”的问题，而是该往哪里发展。是优化性能还是优先扩展功能？是提高用户体验还是保证合规与安全？这些权衡涉及商业逻辑、团队目标与用户价值判断，而这正是人类专家的独特优势。
    

因此，在接下来的内容里，我们将提供一些更细致的指南以提升LLM表现的上限。

## 关联准确的/更多的上下文

您可以把和AI辅助的过程理解为一起打扫房间，你花更多的时间“视察”并给出要打扫的地方，LLM就能把房间的角落清扫地更准确、更彻底。

因此，给LLM准确和更多的上下文是非常有必要的。例如，使用@符号给Cursor的Chat模块更多的代码信息。

此外，你也可以使用 **repomix** 或 **files-to-prompt** 将代码打包注入到 LLM 的上下文窗口，提升代码理解能力。

一个high-level的原则是：**聚焦任务级别，而非整个项目。**因此，你应该分层次提供上下文。以下的分层将项目分为三个层次：

- 顶层：项目目标、核心架构原则
- 中层：相关模块接口、设计模式说明
- 底层：具体函数或测试文件

例如，您可以针对单一功能点编写一个迷你 PRD，然后引导 Cursor Agent 实现它。这种方式就像指导一名初级开发者处理一个 GitHub issue，效果更佳。

## 如何处理错误和 Bug？

关于编程和软件，您必须知道一件事：它们会失败。无论您尝试如何防止，都会发生。所以，我们首先接受它并与错误和 Bug 成为朋友。

这里的第一个策略是模仿软件工程师的做法：查看解释器/编译器给您的错误消息，并尝试理解它。复制并粘贴错误回到 LLM，并要求它给出错误原因并且修复它。另一个好主意是添加像 BrowserTools 这样非常适合调试的 MCP 工具，它们能够自动捕捉、分析并反馈浏览器端的错误，减少人工干预。

## 关注代码风格

以下是我们整理的代码风格建议，这些建议在保持代码可读性的同时最大化帮助 AI 理解并完成代码任务（尤其是在解决上下文爆炸问题上）：

- **最小化不必要的缩进、空格与换行**，保持代码紧凑。
- **顶层函数、类和模块名称**尽量使用完整的命名规范。函数/类/模块内部的代码（尤其是临时变量）应缩短变量名，以减少代码体积。
- **为顶层函数、类和模块提供简要但必要的注释**，说明其目的、输入与输出。函数内部除非必要，不添加注释。
- **尽量在单个文件中实现更多相关的函数、类或模块**。仅在功能模块确实独立时才拆分到不同文件。
- **使用高级语言特性来缩减代码量**，例如在合适场景下优先使用 Lambda 函数代替完整函数定义；利用语法糖；在 C++、TypeScript 等静态类型语言中使用类型推断减少冗长声明。
- **可复用的模块、函数或类应抽象为独立单元**，以便重用并减少整体代码体积。
- **避免重复代码**（如相似逻辑或函数），优先使用高阶函数、装饰器或 mixins 封装通用逻辑。
- **使用第三方库时，优先选择简洁高效的库**，满足功能需求的同时避免过多开销。

## MCP 是什么，我如何从中受益？

MCP 是 模型上下文协议（Model Context Protocol）的缩写。它由 Anthropic 开发，但现在 OpenAI 的 GPT 和 Google 的 Gemini 等其他 LLM 也在使用它。这是一个强大的概念，并且与另一个概念紧密相连：函数/工具调用。
<figure style="margin: 1rem 0; text-align: center;">
  <img src="../img/mcp-simple-diagram.png" alt="MCP" style="display:block; margin:0 auto; max-width:100%;">
  <figcaption style="font-size: 0.9em; color: #666;">图：MCP 的基本交互流程</figcaption>
</figure>

> 为了减少初学者不必要的疑惑，我想特别指出的是：MCP 只是一个抽象概念 (某种协议)，如果你在应用层面提到 MCP 时，你应该知道：你是在使用别人维护的服务 (MCP Servers) 或调用第三方工具 (MCP Tools). 

MCP的想法起源于 工具调用 (Tool Calling)，这是 LLM 调用工具或函数来执行某些操作的一种方式。

它是一种更新 LLM 知识上下文以获取新信息（尤其基于过去陈旧的数据训练），同时将其与外部工具和端点集成的方式。例如，如果您想在网上搜索某些信息，您可以指示 LLM 使用执行此操作的工具（例如：“Hi，如果你需要网页搜索，请使用此工具：search”）。然后，LLM 将调用该工具，获取输出，并在为您生成新预测时使用它，而不是花费大量的令牌、迭代步骤和解析工作负载。

MCP 通过为此 *制定标准* 来扩展这个想法。例如可以创建一个 MCP 服务器，该服务器将向 LLM 公开某些资源（例如数据库）或工具（例如将计算某些内容并返回结果的特定软件）。

创建 MCP Server 并分享出来是一件很有意义的事情，不过我想，我们大部分人更需要知道的事如何使用 MCP Server 吧。下面我将举一个例子，这个例子将 MCP 工具 (Noton MCP) 聚合到 Codex 中使用，以架起代码和文档之间的互相查看和编辑的桥梁。

*注：Notion 是一个界面非常友好和热门的云端知识管理软件，它提供的 MCP Servers 在这里： https://developers.notion.com/docs/mcp。*

注意，这个示例并非随意陈列在这里，而是非常有实际的意义（无论你是在做开发工作还是科研工作）。您将 Notion 当作“需求/决策/记录”的单一真相来源，把代码仓库当作“实现/验证”的单一真相来源，让 Codex 负责在两者之间来回搬运**必要且相关**的上下文，并把产出同步回 Notion，形成闭环。通过这样，您的 读/写需求 → 拆任务 → 局部改动 → 本地验证 → 回写沉淀 的工作节奏将无缝衔接。



👉 **[示例]**  一种通常效果良好的方法/程序/策略/工作流程

打开 Codex 的配置文件（通常在～/.codex/config.toml，或者如果你是 Codex 插件则直接在 Setting 中打开这个文件）

然后修改如下：
```
...
[features]
rmcp_client = true
...

[mcp_servers.notion]
url = "https://mcp.notion.com/mcp"
enabled = true
...
```
因为 Codex 无法热加载，所以你需要重启Codex。

之后在终端输入:
```
codex mcp login notion
```
这将弹出来 Notion 的登录页面，你需要将读写权限给Codex

为了验证是否登录成功，你可以在终端输入：
```
codex mcp list
```
如果显示
```
notion  https://mcp.notion.com/mcp  -   enabled  OAuth  
```
则代表你的 Codex 已经连接 Notion MCP 成功了


之后，你可以直接在 Prompt 中输入：
```
- 在 Notion PRD 页面里写清楚目标、范围、非目标、验收标准（越像 GitHub issue 越好。
- 将本次代码库的改动的摘要、关键决策、TODO、以及如何验证（命令/步骤）写回 PRD 页面或对应的任务条目。
```
至此，你连接Notion MCP 以及 通过 Codex 控制 Notion 的体验就结束了。

除了Notion，还有其他很多专注于娱乐、通信、生产力等厂商都提供了它们的 MCP 服务，
Anthropic 在此维护了一个实时更新的 MCP 服务器列表：https://github.com/modelcontextprotocol/servers

> 不同的 MCP 服务厂商提供不同的方式来允许 Agent 接入。例如，刚刚使用 codex mcp login notion 命令来登录 Notion 是因为 Notion MCP 支持 OAuth 的认证方式；而 Context7 则使用 Bearer token 的方式支持认证，这意味着你无需登录，但需要在Context7 的官方网站获取一个 Bearer Token Key。
>
> 这些信息你都可以从 Agent 厂商（客户端，Client），例如 Codex、Claude Code，或者每个 MCP 服务厂商（服务端，Server）的相关文档中获取.

## 我该如何让AI辅助测试？

是的，测试比以往任何时候都更加重要。在 2025 年的最新技术水平下，LLM 善于生成清晰正确的代码，但它们有时会产生幻觉——更重要的是，它们可能会无法理解规范并生成正确的代码来做错误的事情。

即使我们获得了完全人类水平的通用人工智能，这种情况也不太可能改变——毕竟，人类也会误解规范！语言的模糊性是测试在未来仍然重要的原因。

使用 TDD 来创建您想要的结果骨架可以真正帮助引导 LLM 实现您正在测试的目标代码片段。指示您的 LLM 创建测试并运行它们也是一个很好的实践：它将能够将其发现的可能导致给定测试失败的错误添加到其上下文中，并据此行动，尝试使测试通过。

测试对于与 LLM 一起发展您的代码库至关重要，只有当所有当前测试都通过时才能继续前进。

基于属性的测试（TTD）在与 LLM 合作时非常有趣。测试整个值域/范围而不是仅仅是您指定特定值将有助于确保代理生成的代码即使在后续更改最终触及您未提前考虑到的边缘情况时仍然有效。每种语言都有很好的基于属性的测试库，例如 Python 的 hypothesis 或 JavaScript/TypeScript 的 fast-check。

始终检查 LLM 在尝试编写或修复测试时生成的代码也很重要：有时它们甚至会尝试生成一些硬编程输出以使测试通过 :-)

**如何确保安全？**

这里也适用非 AI 辅助编程的相同规则和最佳实践。研究更多关于它们并将其应用于您的代码。这里是一个初始的安全检查列表：

- **不要相信 AI 生成的代码。** 务必验证。请记住，您运行的代码的责任不在 AI，而在您自己！
- **不要将任何 API 密钥或其他秘密作为硬编程字符串存储，尤其是在前端代码中。** 将它们存储在后端作为受保护的环境变量（例如 Vercel 等平台提供此选项）。
- **查询 API 端点时，始终使用 HTTPS。**
- **创建 HTML 表单时，始终进行输入验证和清理。**
- **不要将敏感数据存储在 localStorage、sessionStorage 或 cookies 中。**
- **在您的包依赖项中运行验证器和安全漏洞扫描器。**
---
layout: default
title: 远离纯粹的Vibe coding - AI作为副驾驶
nav_order: 4
---
假设现在你已经使用AI成功构建好了你软件项目的第一个版本，你使用PRD、子任务文档和预设的规则幸运地在几次或一次交互中生成代码、解决bug/issue、启动服务。

但是不幸的是，至此，您纯粹的 Vibe coding 之旅就要结束了。之后，*你必须逐渐花费更多的脑力从AI手里夺过方向盘，坐到主驾驶的位置。*

我们希望通过分享下面的几点或深层或显而易见的思考，来说明你必须让 AI 回到副驾驶位置的原因：

> 🌑思考-1
>1. **上下文窗口永远显得不足**   
>    无论是 32k、64k、256k、100 万 tokens，还是更大，提示工程师们总会希望塞入更多代码与上下文。过去我们只能放两三个文件，将来可能会希望一次性放入几十甚至上百个文件。
>    你不可能把整个代码库的代码全部给LLM，而LLM也很难准确地检索到你想要修改或者审查的代码位置。   
>2. **LLM在软件设计上远达不到人类专家水平**
>    这一方面来自于LLM上下文窗口不足（无法看到并分析代码库的全貌），另一方面源于其缺乏真正的架构性思维与抽象能力。   
>    人类专家在软件设计时会考虑长期演进、团队协作、可维护性、性能权衡、安全性与合规性等多重因素，而 LLM 更像是基于已有数据和模式的“局部补全器”。这意味着 LLM 擅长的是战术层面的代码生成与修改，而在战略层面的架构设计、模式选型、需求折中、权衡取舍方面，它仍然无法达到人类的深度与远见，结果是：LLM 生成的代码往往可用，但可能导致架构上的碎片化、重复造轮子、缺乏一致性和统一标准。
>3. **维持项目稳定并持续测试和演进需要人来主导**
>    一方面，AI 的幻觉和输出的不确定性，将轻易地破坏项目的稳定性，例如已经实现的功能。尤其是当我们把修改、测试、执行的权限AI全部交给AI时，这种担忧更为突出。（例如，为了测试通过，（绝大部分）LLM都倾向于给很简单的测试用例或者直接修改被测试代码来适配测试用例🫠）   
>    另一方面，项目后期面临的不再是“能不能跑起来”的问题，而是该往哪里发展。是优化性能还是优先扩展功能？是提高用户体验还是保证合规与安全？这些权衡涉及商业逻辑、团队目标与用户价值判断，而这正是人类专家的独特优势。
    

因此，在接下来的内容里，我们将提供一些更细致的指南以提升LLM表现的上限。

## 关联准确的/更多的上下文

您可以把和AI辅助的过程理解为一起打扫房间，你花更多的时间“视察”并给出要打扫的地方，LLM就能把房间的角落清扫地更准确、更彻底。

因此，给LLM准确和更多的上下文是非常有必要的。例如，使用@符号给Cursor的Chat模块更多的代码信息。

此外，你也可以使用 **repomix** 或 **files-to-prompt** 将代码打包注入到 LLM 的上下文窗口，提升代码理解能力。

一个high-level的原则是：**聚焦任务级别，而非整个项目。**因此，你应该分层次提供上下文。以下的分层将项目分为三个层次：

- 顶层：项目目标、核心架构原则
- 中层：相关模块接口、设计模式说明
- 底层：具体函数或测试文件

例如，您可以针对单一功能点编写一个迷你 PRD，然后引导 Cursor Agent 实现它。这种方式就像指导一名初级开发者处理一个 GitHub issue，效果更佳。

## 如何处理错误和 Bug？

关于编程和软件，您必须知道一件事：它们会失败。无论您尝试如何防止，都会发生。所以，我们首先接受它并与错误和 Bug 成为朋友。

这里的第一个策略是模仿软件工程师的做法：查看解释器/编译器给您的错误消息，并尝试理解它。复制并粘贴错误回到 LLM，并要求它给出错误原因并且修复它。另一个好主意是添加像 BrowserTools 这样非常适合调试的 MCP 工具，它们能够自动捕捉、分析并反馈浏览器端的错误，减少人工干预。

## 关注代码风格

以下是我们整理的代码风格建议，这些建议在保持代码可读性的同时最大化帮助 AI 理解并完成代码任务（尤其是在解决上下文爆炸问题上）：

- **最小化不必要的缩进、空格与换行**，保持代码紧凑。
- **顶层函数、类和模块名称**尽量使用完整的命名规范。函数/类/模块内部的代码（尤其是临时变量）应缩短变量名，以减少代码体积。
- **为顶层函数、类和模块提供简要但必要的注释**，说明其目的、输入与输出。函数内部除非必要，不添加注释。
- **尽量在单个文件中实现更多相关的函数、类或模块**。仅在功能模块确实独立时才拆分到不同文件。
- **使用高级语言特性来缩减代码量**，例如在合适场景下优先使用 Lambda 函数代替完整函数定义；利用语法糖；在 C++、TypeScript 等静态类型语言中使用类型推断减少冗长声明。
- **可复用的模块、函数或类应抽象为独立单元**，以便重用并减少整体代码体积。
- **避免重复代码**（如相似逻辑或函数），优先使用高阶函数、装饰器或 mixins 封装通用逻辑。
- **使用第三方库时，优先选择简洁高效的库**，满足功能需求的同时避免过多开销。

## MCP、SLOP 和 A2A 是什么，我如何从中受益？

MCP 是模型上下文协议（Model Context Protocol）的缩写。它由 Anthropic 开发，但现在 OpenAI 的 GPT 和 Google 的 Gemini 等其他 LLM 也开始考虑使用它。这是一个强大的概念，并且与另一个概念紧密相连：函数/工具调用。

工具调用是 LLM 调用工具或函数来执行某些操作的一种方式。它是一种更新 LLM 知识窗口（基于过去数据训练）以获取新信息，同时将其与外部工具和端点集成的方式。例如，如果您想在网上搜索某些信息，您可以指示 LLM 使用执行此操作的工具（例如：“嘿，如果您需要在网上搜索，请使用此工具：search(term)”）。然后，LLM 将调用该工具，获取输出，并在为您生成新预测时使用它，而不是花费大量的令牌、迭代步骤和解析工作负载。

MCP 通过为此创建标准来扩展这个想法。这样我们就可以创建一个 MCP 服务器，该服务器将向 LLM 公开某些资源（例如数据库）或工具（例如将计算某些内容并返回结果的特定软件）。

等等，但这不只是一个 API 吗？我不能用 REST API 服务器/客户端和 LLM 提示中的一些解析来模拟同样的事情吗？有点像，这也是 SLOP（Simple Language Open Protocol）所提出的。然而，拥有像 MCP 这样的标准使得确保 LLM 原生支持它而无需客户端进行额外的解析和技巧变得更容易。

A2A（Agent to Agent Protocol）在游戏中非常新。它由 Google 创建，旨在“补充”MCP，专注于多代理通信，而 MCP 则专注于 LLM-工具通信。

**重要提示：** 有很多优秀的 MCP 服务器，Cursor 等编辑器也支持它们。目前，只有 Anthropic Claude LLM 支持它们，因此当您想使用 MCP 工具时，请确保使用 Claude。

Anthropic 在此维护了一个更新的 MCP 服务器列表：https://github.com/modelcontextprotocol/servers

## 我该如何让AI辅助测试？

是的，测试比以往任何时候都更加重要。在 2025 年的最新技术水平下，LLM 善于生成清晰正确的代码，但它们有时会产生幻觉——更重要的是，它们可能会无法理解规范并生成正确的代码来做错误的事情。

即使我们获得了完全人类水平的通用人工智能，这种情况也不太可能改变——毕竟，人类也会误解规范！语言的模糊性是测试在未来仍然重要的原因。

使用 TDD 来创建您想要的结果骨架可以真正帮助引导 LLM 实现您正在测试的目标代码片段。指示您的 LLM 创建测试并运行它们也是一个很好的实践：它将能够将其发现的可能导致给定测试失败的错误添加到其上下文中，并据此行动，尝试使测试通过。

测试对于与 LLM 一起发展您的代码库至关重要，只有当所有当前测试都通过时才能继续前进。

基于属性的测试（TTD）在与 LLM 合作时非常有趣。测试整个值域/范围而不是仅仅是您指定特定值将有助于确保代理生成的代码即使在后续更改最终触及您未提前考虑到的边缘情况时仍然有效。每种语言都有很好的基于属性的测试库，例如 Python 的 hypothesis 或 JavaScript/TypeScript 的 fast-check。

始终检查 LLM 在尝试编写或修复测试时生成的代码也很重要：有时它们甚至会尝试生成一些硬编程输出以使测试通过 :-)

**如何确保安全？**

这里也适用非 AI 辅助编程的相同规则和最佳实践。研究更多关于它们并将其应用于您的代码。这里是一个初始的安全检查列表：

- **不要相信 AI 生成的代码。** 务必验证。请记住，您运行的代码的责任不在 AI，而在您自己！
- **不要将任何 API 密钥或其他秘密作为硬编程字符串存储，尤其是在前端代码中。** 将它们存储在后端作为受保护的环境变量（例如 Vercel 等平台提供此选项）。
- **查询 API 端点时，始终使用 HTTPS。**
- **创建 HTML 表单时，始终进行输入验证和清理。**
- **不要将敏感数据存储在 localStorage、sessionStorage 或 cookies 中。**
- **在您的包依赖项中运行验证器和安全漏洞扫描器。**